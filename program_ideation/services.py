import logging
from django.conf import settings
from askme.services import LLMService

logger = logging.getLogger(__name__)


class ProgramIdeationService:
    """Service for program ideation using LLM."""
    
    def __init__(self, language='ar'):
        self.language = language
        self.llm_service = LLMService()
    
    def get_idea_suggestions(self):
        """Get program idea suggestions."""
        if self.language == 'ar':
            prompt = """Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø§Ù‚ØªØ±Ø­ 5 Ø£ÙÙƒØ§Ø± Ù…Ø¨ØªÙƒØ±Ø© Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø¬Ø¯ÙŠØ¯Ø© ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ ÙˆØªØªÙ…Ø§Ø´Ù‰ Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠ. Ù…Ù„Ø§Ø­Ø¸Ø© Ø±Ø¬Ø§Ø¡Ø§ Ø¹Ø¯Ù… ÙƒØªØ§Ø¨Ø© Ø¹Ù„Ø§Ù…Ø© * Ùˆ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªÙ†ØµÙŠØµ "".

Ù‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§ØªÙƒ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:

===== Ø§Ø³Ù… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø£ÙˆÙ„]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø«Ø§Ù†ÙŠ]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø«Ø§Ù„Ø«]
Ø§Ù‚ØªØ±Ø§Ø­ 4: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø±Ø§Ø¨Ø¹]
Ø§Ù‚ØªØ±Ø§Ø­ 5: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø®Ø§Ù…Ø³]

===== Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 4: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 5: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø®Ø§Ù…Ø³Ø©]

===== Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 4: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 5: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø®Ø§Ù…Ø³Ø©]

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ø¨Ø®Ù…Ø³Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø³Ø§ÙˆØ§Ø© (=====) Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ø§Ø³Ù… Ø§Ù„Ø­Ù‚Ù„. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = """You are a creative consultant for the Saudi Broadcasting Authority. Suggest 5 innovative ideas for new programs suitable for the Kingdom of Saudi Arabia, in line with the current media context.

Provide your suggestions in the following format:

===== Program Name =====
Suggestion 1: [first proposed program title]
Suggestion 2: [second proposed program title]
Suggestion 3: [third proposed program title]
Suggestion 4: [fourth proposed program title]
Suggestion 5: [fifth proposed program title]

===== General Idea =====
Suggestion 1: [brief description of the first idea]
Suggestion 2: [brief description of the second idea]
Suggestion 3: [brief description of the third idea]
Suggestion 4: [brief description of the fourth idea]
Suggestion 5: [brief description of the fifth idea]

===== Target Audience =====
Suggestion 1: [description of target audience for the first idea]
Suggestion 2: [description of target audience for the second idea]
Suggestion 3: [description of target audience for the third idea]
Suggestion 4: [description of target audience for the fourth idea]
Suggestion 5: [description of target audience for the fifth idea]

Use the exact formatting as shown, keeping the headings marked with five equals signs (=====) before and after the field name."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error generating idea suggestions: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in get_idea_suggestions: {str(e)}")
            return self._get_error_message()
    
    def process_initial_concept(self, concept):
        """Process initial concept and provide suggestions."""
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙˆØ± Ø§Ù„Ø£ÙˆÙ„ÙŠ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø§Ù‚ØªØ±Ø­ 5 Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø®ØªÙ„ÙØ© ÙˆÙ†Ø·Ø§Ù‚ Ù„Ù„ÙÙƒØ±Ø©. Ù…Ù„Ø§Ø­Ø¸Ø© Ø±Ø¬Ø§Ø¡Ø§ Ø¹Ø¯Ù… ÙƒØªØ§Ø¨Ø© Ø¹Ù„Ø§Ù…Ø© * Ùˆ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªÙ†ØµÙŠØµ "".:

Ø§Ù„ØªØµÙˆØ± Ø§Ù„Ø£ÙˆÙ„ÙŠ: {concept}

Ù‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§ØªÙƒ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:

===== Ø§Ø³Ù… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø£ÙˆÙ„]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø«Ø§Ù†ÙŠ]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø«Ø§Ù„Ø«]
Ø§Ù‚ØªØ±Ø§Ø­ 4: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø±Ø§Ø¨Ø¹]
Ø§Ù‚ØªØ±Ø§Ø­ 5: [Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø®Ø§Ù…Ø³]

===== Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 4: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 5: [ÙˆØµÙ Ù…ÙˆØ¬Ø² Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø®Ø§Ù…Ø³Ø©]

===== Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 4: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©]
Ø§Ù‚ØªØ±Ø§Ø­ 5: [ÙˆØµÙ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ÙÙƒØ±Ø© Ø§Ù„Ø®Ø§Ù…Ø³Ø©]


Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ø¨Ø®Ù…Ø³Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø³Ø§ÙˆØ§Ø© (=====) Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ø§Ø³Ù… Ø§Ù„Ø­Ù‚Ù„. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. Based on the following initial concept, suggest different titles and scope for the idea:

Initial concept: {concept}

Provide your suggestions in the following format:

===== Program Name =====
Suggestion 1: [proposed program title]
Suggestion 2: [proposed program title]
Suggestion 3: [proposed program title]
Suggestion 4: [proposed program title]
Suggestion 5: [proposed program title]

===== General Idea =====
Suggestion 1: [brief description of the idea]
Suggestion 2: [brief description of the idea]
Suggestion 3: [brief description of the idea]

===== Target Audience =====
Suggestion 1: [description of target audience]
Suggestion 2: [description of target audience]

Use the exact formatting as shown, keeping the headings marked with five equals signs (=====) before and after the field name."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error processing initial concept: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in process_initial_concept: {str(e)}")
            return self._get_error_message()
    
    def get_missing_data_proposals(self, idea):
        """Get proposals for missing data."""
        missing_fields = idea.get_missing_fields()
        
        # Translate field names to Arabic labels for prompt
        field_labels = {
            'program_name': 'Ø§Ø³Ù… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬',
            'general_idea': 'Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø©',
            'target_audience': 'Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù',
            'program_objectives': 'Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬',
            'program_type': 'Ù†ÙˆØ¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬',
            'program_duration': 'Ù…Ø¯Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬',
            'episode_count': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø§Øª',
            'filming_location': 'Ù…ÙˆÙ‚Ø¹ Ø£Ùˆ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªØµÙˆÙŠØ±'
        }
        
        # Create prompt based on available data
        available_data = ""
        for field in ['program_name', 'general_idea', 'target_audience', 'program_objectives', 
                      'program_type', 'program_duration', 'episode_count', 'filming_location']:
            if field not in missing_fields and getattr(idea, field):
                available_data += f"{field_labels[field]}: {getattr(idea, field)}\n"
        
        missing_fields_labels = [field_labels[field] for field in missing_fields]
        missing_fields_str = ", ".join(missing_fields_labels)
        
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¹Ù† ÙÙƒØ±Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŒ Ø§Ù‚ØªØ±Ø­ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†Ø§Ù‚ØµØ©. Ù…Ù„Ø§Ø­Ø¸Ø© Ø±Ø¬Ø§Ø¡Ø§ Ø¹Ø¯Ù… ÙƒØªØ§Ø¨Ø© Ø¹Ù„Ø§Ù…Ø© * Ùˆ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªÙ†ØµÙŠØµ "".

Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©:
{available_data}

Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†Ø§Ù‚ØµØ©: {missing_fields_str}

Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„ÙƒÙ„ Ø­Ù‚Ù„ Ù†Ø§Ù‚Øµ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ ÙÙ‚Ø· Ù„Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†Ø§Ù‚ØµØ©!!:

===== {field_labels[missing_fields[0]]} =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]

{f"===== {field_labels[missing_fields[1]]} =====" if len(missing_fields) > 1 else ""}
{f"Ø§Ù‚ØªØ±Ø§Ø­ 1: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]" if len(missing_fields) > 1 else ""}
{f"Ø§Ù‚ØªØ±Ø§Ø­ 2: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]" if len(missing_fields) > 1 else ""}

ÙˆÙ‡ÙƒØ°Ø§ Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†Ø§Ù‚ØµØ©. Ù‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­ÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„ÙƒÙ„ Ø­Ù‚Ù„ Ù†Ø§Ù‚ØµØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù†Ø³Ø¬Ø§Ù…Ù‡Ø§ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ø¨Ø®Ù…Ø³Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø³Ø§ÙˆØ§Ø© (=====) Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ø§Ø³Ù… Ø§Ù„Ø­Ù‚Ù„. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. Based on the available information about the program idea, suggest data for the missing fields.

Available data:
{available_data}

Missing fields: {missing_fields_str}

Provide suggestions for each missing field in the following format:

===== {field_labels[missing_fields[0]]} =====
Suggestion 1: [suggested text for the field]
Suggestion 2: [suggested text for the field]

{f"===== {field_labels[missing_fields[1]]} =====" if len(missing_fields) > 1 else ""}
{f"Suggestion 1: [suggested text for the field]" if len(missing_fields) > 1 else ""}
{f"Suggestion 2: [suggested text for the field]" if len(missing_fields) > 1 else ""}

And so on for the remaining missing fields. Provide at least two different suggestions for each missing field, ensuring consistency with the available data.

Use the exact formatting as shown, keeping the headings marked with five equals signs (=====) before and after the field name."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error generating missing data proposals: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in get_missing_data_proposals: {str(e)}")
            return self._get_error_message()
    
    def generate_discussion_questions(self, idea):
        """Generate discussion questions for the idea workshop."""
        # Collect all available data about the idea
        idea_data = ""
        for field in ['program_name', 'general_idea', 'target_audience', 'program_objectives', 
                      'program_type', 'program_duration', 'episode_count', 'filming_location']:
            if getattr(idea, field):
                idea_data += f"{field}: {getattr(idea, field)}\n"
        
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­ØŒ Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù†Ù‚Ø§Ø´ Ù„ÙˆØ±Ø´Ø© Ø¹Ù…Ù„ Ù„Ù…Ù†Ø§Ù‚Ø´Ø© ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„ÙÙƒØ±Ø©.

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬:
{idea_data}

Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ 10 Ø£Ø³Ø¦Ù„Ø© Ù†Ù‚Ø§Ø´ Ù…Ù‡Ù…Ø© ØªØ³Ø§Ø¹Ø¯ ÙÙŠ:
1. Ø§Ø³ØªÙƒØ´Ø§Ù Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„ÙÙƒØ±Ø© Ø¨Ø¹Ù…Ù‚
2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙˆÙƒÙŠÙÙŠØ© Ø§Ù„ØªØºÙ„Ø¨ Ø¹Ù„ÙŠÙ‡Ø§
3. ØªØ·ÙˆÙŠØ± Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
4. Ø¶Ù…Ø§Ù† Ø¬Ø§Ø°Ø¨ÙŠØ© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù

Ù‚Ø¯Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙÙŠ ØªÙ†Ø³ÙŠÙ‚ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¸Ù…ØŒ Ù…Ø¹ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ù…Ø­Ø§ÙˆØ± Ù…Ù†Ø§Ø³Ø¨Ø©. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø³Ø¤Ø§Ù„ Ø§Ùˆ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. Based on the proposed program information, prepare discussion questions for a workshop to discuss and develop the idea.

Program information:
{idea_data}

Prepare 10 important discussion questions that help:
1. Explore aspects of the idea in depth
2. Identify potential challenges and how to overcome them
3. Develop program elements
4. Ensure the program's appeal to the target audience

Present the questions in a clear, organized format, dividing them into appropriate axes."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error generating discussion questions: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in generate_discussion_questions: {str(e)}")
            return self._get_error_message()
    
    def generate_program_format(self, idea):
        """Generate program format."""
        # Collect all available data about the idea
        idea_data = ""
        for field in ['program_name', 'general_idea', 'target_audience', 'program_objectives', 
                      'program_type', 'program_duration', 'episode_count', 'filming_location']:
            if getattr(idea, field):
                idea_data += f"{field}: {getattr(idea, field)}\n"
        
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŒ Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ†Ø³ÙŠÙ‚ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬.

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬:
{idea_data}

Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…ØªØ¶Ù…Ù†Ø§Ù‹:
1. Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ (ÙÙ‚Ø±Ø§Øª/Ø£Ù‚Ø³Ø§Ù…)
2. ØªÙØ§ØµÙŠÙ„ ÙƒÙ„ ÙÙ‚Ø±Ø© ÙˆÙ‡Ø¯ÙÙ‡Ø§
3. Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„Ù„ÙÙ‚Ø±Ø§Øª
4. Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„ÙƒÙ„ ÙÙ‚Ø±Ø©
5. Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
6. Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ù‚ØªØ±Ø­

Ù‚Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø´ÙƒÙ„ ØªÙØµÙŠÙ„ÙŠ ÙˆÙ…Ù†Ø¸Ù…. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. Based on the program information, prepare a detailed format for the program.

Program information:
{idea_data}

Prepare the program format including:
1. Program structure (segments/sections)
2. Details of each segment and its purpose
3. Logical sequence of segments
4. Proposed timing for each segment
5. Key visual elements
6. Proposed presentation style

Present the format in a detailed and organized manner."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error generating program format: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in generate_program_format: {str(e)}")
            return self._get_error_message()
    
    def generate_program_script(self, idea):
        """Generate program script."""
        # Collect all available data about the idea
        idea_data = ""
        for field in ['program_name', 'general_idea', 'target_audience', 'program_objectives', 
                      'program_type', 'program_duration', 'episode_count', 'filming_location']:
            if getattr(idea, field):
                idea_data += f"{field}: {getattr(idea, field)}\n"
        
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŒ Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù†Øµ Ø­Ù„Ù‚Ø© Ù…Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬.

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬:
{idea_data}

Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Øµ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…ØªØ¶Ù…Ù†Ø§Ù‹:
1. Ù…Ù‚Ø¯Ù…Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
2. Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…/Ø§Ù„Ù…Ù‚Ø¯Ù…ÙŠÙ†
3. ØªØ³Ù„Ø³Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª
4. Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
5. Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª Ø¨ÙŠÙ† Ø§Ù„ÙÙ‚Ø±Ø§Øª
6. Ø®Ø§ØªÙ…Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬

Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¨Ø« Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø·Ø¨ÙŠØ¹Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙˆØ¬Ù…Ù‡ÙˆØ±Ù‡ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. Based on the program information, prepare a model script for an episode of the program.

Program information:
{idea_data}

Prepare the program script including:
1. Program introduction
2. Presenter(s) scripts
3. Sequence of segments
4. Examples of potential dialogues
5. Transitions between segments
6. Program conclusion

Write the script professionally, suitable for television broadcasting, taking into account the nature of the program and its target audience."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error generating program script: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in generate_program_script: {str(e)}")
            return self._get_error_message()
    
    def generate_visual_proposals(self, idea):
        """Generate visual material proposals."""
        # Collect all available data about the idea
        idea_data = ""
        for field in ['program_name', 'general_idea', 'target_audience', 'program_objectives', 
                      'program_type', 'program_duration', 'episode_count', 'filming_location']:
            if getattr(idea, field):
                idea_data += f"{field}: {getattr(idea, field)}\n"
        
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŒ Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ù…Ù‚ØªØ±Ø­Ø§Øª Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬.

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬:
{idea_data}

Ù‚Ø¯Ù… Ù…Ù‚ØªØ±Ø­Ø§Øª Ù…ÙØµÙ„Ø© Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. Ø´Ø¹Ø§Ø± Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ (ÙˆØµÙ Ø§Ù„ØªØµÙ…ÙŠÙ…ØŒ Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŒ Ø§Ù„Ø¹Ù†Ø§ØµØ±)
2. ØªØµÙ…ÙŠÙ… Ø§Ù„Ø§Ø³ØªÙˆØ¯ÙŠÙˆ/Ù…ÙˆÙ‚Ø¹ Ø§Ù„ØªØµÙˆÙŠØ± (Ø§Ù„Ø¯ÙŠÙƒÙˆØ±ØŒ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©ØŒ Ø§Ù„Ø£Ø«Ø§Ø«)
3. Ø§Ù„Ø¬Ø±Ø§ÙÙŠÙƒØ³ (Ø£Ø³Ù„ÙˆØ¨ØŒ Ø£Ù„ÙˆØ§Ù†ØŒ Ø¹Ù†Ø§ØµØ± Ù…ØªØ­Ø±ÙƒØ©)
4. Ø§Ù„ÙÙˆØ§ØµÙ„ ÙˆØ§Ù„Ù…Ù‚Ø¯Ù…Ø© (ÙˆØµÙ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªØµÙˆÙŠØ±ØŒØ§Ù„Ø£Ø³Ù„ÙˆØ¨)
5. Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø§Ù„ÙØ±ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ØªÙ…ÙŠØ² Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬

Ù‚Ø¯Ù… Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø§Øª Ø¨Ø´ÙƒÙ„ ØªÙØµÙŠÙ„ÙŠ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø·Ø¨ÙŠØ¹Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙˆØ¬Ù…Ù‡ÙˆØ±Ù‡ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. Based on the program information, provide proposals for the visual materials of the program.

Program information:
{idea_data}

Provide detailed proposals for the following visual elements:
1. Program logo (design description, colors, elements)
2. Studio/filming location design (decor, lighting, furniture)
3. Graphics (style, colors, moving elements)
4. Breaks and introduction (description of filming style, style)
5. Unique visual elements that distinguish the program

Present the proposals in detail, taking into account the nature of the program and its target audience."""
        
        # Use the default (first) LLM model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error generating visual proposals: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in generate_visual_proposals: {str(e)}")
            return self._get_error_message()
    
    def process_idea_note(self, note):
        """Process a note and generate enhancement suggestions."""
        # Get the field content
        idea = note.idea
        field_name = note.field_name
        field_content = getattr(idea, field_name, "")
        
        # Get field label
        field_label = note.get_field_name_display()
        if self.language == 'ar':
            field_label = note.get_field_label_arabic()
        
        # Prepare prompt based on language
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±ØºØ¨ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ù…Ø­ØªÙˆÙ‰ Ø­Ù‚Ù„. Ù…Ù„Ø§Ø­Ø¸Ø© Ø±Ø¬Ø§Ø¡Ø§ Ø¹Ø¯Ù… ÙƒØªØ§Ø¨Ø© Ø¹Ù„Ø§Ù…Ø© * Ùˆ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªÙ†ØµÙŠØµ "". "{field_label}" ÙÙŠ ÙÙƒØ±Ø© Ø¨Ø±Ù†Ø§Ù…Ø¬.

Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø­Ø§Ù„ÙŠ: {field_content}

Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {note.note_content}

Ù‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ø§Ù„Ø­Ù‚Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:

===== {field_label} =====
Ø§Ù‚ØªØ±Ø§Ø­ 1: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]
Ø§Ù‚ØªØ±Ø§Ø­ 2: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]
Ø§Ù‚ØªØ±Ø§Ø­ 3: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]

ÙƒÙ„ Ø§Ù‚ØªØ±Ø§Ø­ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø³ØªÙ‚Ù„Ø§Ù‹ Ø¨Ø°Ø§ØªÙ‡ ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© ÙƒÙ…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø­Ù‚Ù„.
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù…ÙŠØ² Ø¨Ø®Ù…Ø³Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø³Ø§ÙˆØ§Ø© (=====) Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ø§Ø³Ù… Ø§Ù„Ø­Ù‚Ù„. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority. The user wants to enhance the content of the "{field_label}" field in a program idea.

Current content: {field_content}

User's note: {note.note_content}

Provide suggestions to enhance this field based on the user's note using the following format:

===== {field_label} =====
Suggestion 1: [suggested text for the field]
Suggestion 2: [suggested text for the field]
Suggestion 3: [suggested text for the field]

Each suggestion should be self-contained and can be used directly as new content for the field.
Use the exact formatting as shown, keeping the heading marked with five equals signs (=====) before and after the field name."""
        
        # Use the default model
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                logger.error(f"Error processing note: {result.get('error')}")
                return self._get_error_message()
        except Exception as e:
            logger.error(f"Exception in process_idea_note: {str(e)}")
            return self._get_error_message()
    
    # def _get_default_model(self):
    #     """Get the default LLM model to use."""
    #     from askme.models import LLMModel
        
    #     # Try to get a model that's meant for this type of creative work
    #     try:
    #         # First try to get an active DeepSeek model specifically
    #         model = LLMModel.objects.filter(is_active=True, provider__iexact='OpenAI').first()
            
    #         if not model:
    #             # Then try OpenAI as fallback 
    #             model = LLMModel.objects.filter(is_active=True, provider__iexact='deepseek').first()
                
    #         if not model:
    #             # Fall back to any active model
    #             model = LLMModel.objects.filter(is_active=True).first()
                
    #         if not model:
    #             # If no active models, get any model
    #             model = LLMModel.objects.first()
            
    #         # Log which model we're using (helpful for debugging)
    #         if model:
    #             logger.info(f"Program Ideation using model: {model.name} ({model.provider})")
            
    #         return model
    #     except Exception as e:
    #         logger.error(f"Error getting default model: {str(e)}")
    #         # Return a mock model for emergency fallback
    #         from types import SimpleNamespace
    #         return SimpleNamespace(provider='Mock', model_id='mock-model')


    # Replace your _get_default_model method in program_ideation/services.py with this debug version:

    def _get_default_model(self):
        """Get the default LLM model with debugging."""
        from askme.models import LLMModel
        
        try:
            # Debug: Log all available models
            all_models = LLMModel.objects.all()
            active_models = LLMModel.objects.filter(is_active=True)
            
            logger.info(f"DEBUG Program Ideation: Total models in database: {all_models.count()}")
            logger.info(f"DEBUG Program Ideation: Active models: {active_models.count()}")
            
            for model in active_models:
                logger.info(f"DEBUG Program Ideation: Available active model: {model.name} ({model.provider}/{model.model_id})")
            
            # Try to get models in priority order
            model_preferences = [
                {'provider': 'openai', 'model_prefix': 'gpt-5', 'name': 'GPT-5'},
                {'provider': 'openai', 'model_prefix': 'gpt-4', 'name': 'GPT-4'},
                {'provider': 'openai', 'name': 'Any OpenAI'},
                {'provider': 'deepseek', 'name': 'DeepSeek'},
            ]
            
            for preference in model_preferences:
                logger.info(f"DEBUG Program Ideation: Trying preference: {preference['name']}")
                
                filters = {'is_active': True, 'provider__iexact': preference['provider']}
                
                # If there's a model prefix, prioritize those models
                if 'model_prefix' in preference:
                    models = LLMModel.objects.filter(
                        **filters,
                        model_id__startswith=preference['model_prefix']
                    ).order_by('name')
                    
                    if models.exists():
                        model = models.first()
                        logger.info(f"DEBUG Program Ideation: âœ… Selected model: {model.name} ({model.provider}/{model.model_id})")
                        return model
                    else:
                        logger.info(f"DEBUG Program Ideation: âŒ No models found for prefix: {preference['model_prefix']}")
                
                # Fall back to any model from this provider
                model = LLMModel.objects.filter(**filters).first()
                if model:
                    logger.info(f"DEBUG Program Ideation: âœ… Selected fallback model: {model.name} ({model.provider}/{model.model_id})")
                    return model
                else:
                    logger.info(f"DEBUG Program Ideation: âŒ No active models for provider: {preference['provider']}")
            
            # Final fallbacks
            model = LLMModel.objects.filter(is_active=True).first()
            if model:
                logger.info(f"DEBUG Program Ideation: âš ï¸ Using any active model: {model.name} ({model.provider})")
                return model
                
            model = LLMModel.objects.first()
            if model:
                logger.warning(f"DEBUG Program Ideation: âš ï¸ Using inactive model: {model.name} ({model.provider})")
                return model
            
            # Emergency mock model
            logger.error("DEBUG Program Ideation: ğŸš¨ No models found - using mock model")
            from types import SimpleNamespace
            return SimpleNamespace(provider='Mock', model_id='mock-model', name='Emergency Mock Model')
            
        except Exception as e:
            logger.error(f"DEBUG Program Ideation: ğŸš¨ Error getting default model: {str(e)}")
            import traceback
            logger.error(f"DEBUG Program Ideation: Full traceback: {traceback.format_exc()}")
            from types import SimpleNamespace
            return SimpleNamespace(provider='Mock', model_id='mock-model', name='Error Fallback Mock Model')
    
    def _get_error_message(self):
        """Get error message based on language."""
        if self.language == 'ar':
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹."
        else:
            return "Sorry, an error occurred while processing your request. Please try again later."
        
        # 3. UPDATE services.py - Add this method to ProgramIdeationService class
    # ============================================

    def process_multi_field_note(self, note):
        """Process a note that references multiple fields."""
        idea = note.idea
        
        # Get all fields associated with this note
        fields_info = []
        for field in note.note_fields.all():
            field_label = field.get_field_label_arabic() if self.language == 'ar' else field.get_field_name_display()
            fields_info.append(f"{field_label}: {field.current_content or '(ÙØ§Ø±Øº)' if self.language == 'ar' else '(Empty)'}")
        
        fields_text = "\n".join(fields_info)
        
        if self.language == 'ar':
            prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø°Ø§Ø¹Ø© ÙˆØ§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ù…Ù„Ø§Ø­Ø¸Ø© Ø±Ø¬Ø§Ø¡Ø§ Ø¹Ø¯Ù… ÙƒØªØ§Ø¨Ø© Ø¹Ù„Ø§Ù…Ø© * Ùˆ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªÙ†ØµÙŠØµ "". 
                                                        
    Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø¯ÙŠÙ‡ Ù…Ù„Ø§Ø­Ø¸Ø© Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø­Ù‚ÙˆÙ„ Ù…Ù† ÙÙƒØ±Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬:

    Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø¹Ù†ÙŠØ©:
    {fields_text}

    Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {note.note_content}

    Ù‚Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ØªØ­Ø³ÙŠÙ† Ù…Ù†ÙØµÙ„Ø© Ù„ÙƒÙ„ Ø­Ù‚Ù„ Ù…Ø¹Ù†ÙŠ Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø­Ù‚ÙˆÙ„. ÙÙ‚Ø· Ù„Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø¹Ù†ÙŠØ©.

    Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ÙƒÙ„ Ø­Ù‚Ù„:

    ===== [Ø§Ø³Ù… Ø§Ù„Ø­Ù‚Ù„] =====
    Ø§Ù‚ØªØ±Ø§Ø­ 1: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]
    Ø§Ù‚ØªØ±Ø§Ø­ 2: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]
    Ø§Ù‚ØªØ±Ø§Ø­ 3: [Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù„Ù„Ø­Ù‚Ù„]

    Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ø¨Ø®Ù…Ø³Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø³Ø§ÙˆØ§Ø©. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ !!."""
        else:
            prompt = f"""You are a creative consultant for the Saudi Broadcasting Authority.

    The user has a note on multiple fields of a program idea:

    Related fields:
    {fields_text}

    Note type: {note.get_note_type_display() if hasattr(note, 'get_note_type_display') else 'Enhancement'}
    Priority: {note.get_priority_display() if hasattr(note, 'get_priority_display') else 'Medium'}

    User's note: {note.note_content}

    Provide separate improvement suggestions for each field mentioned in the note, considering the relationship between fields.

    Use the following format for each field:

    ===== [Field Name] =====
    Suggestion 1: [suggested text for the field]
    Suggestion 2: [suggested text for the field]
    Suggestion 3: [suggested text for the field]

    Use the exact formatting as shown, keeping the headings marked with five equals signs."""
        
        # Get LLM response
        model = self._get_default_model()
        
        try:
            result = self.llm_service.generate_response(
                provider=model.provider,
                model_id=model.model_id,
                prompt=prompt
            )
            
            if result['success']:
                return result['content']
            else:
                return self._get_error_message()
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"Exception in process_multi_field_note: {str(e)}")
            return self._get_error_message()